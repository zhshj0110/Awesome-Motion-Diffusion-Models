# Awesome-Motion-Diffusion-Models

[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat-square)](http://makeapullrequest.com)


We collect existing papers on human motion diffusion models published in prominent conferences and journals. 

This paper list will be continuously updated.

## Table of Contents

- [Datasets](#datasets)
- [Survey](#survey)
- [Papers](#papers)
  - [2024](#2024)
  - [2023](#2023)
  - [2022](#2022)
- [Other Resources](#other-resources)

## Datasets
### Text to Motion

- Generating Diverse and Natural 3D Human Motions from Text (**CVPR 2022**) [[project](https://ericguo5513.github.io/text-to-motion/)] [[paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Guo_Generating_Diverse_and_Natural_3D_Human_Motions_From_Text_CVPR_2022_paper.pdf)] [[code](https://github.com/EricGuo5513/text-to-motion)]


- BABEL: Bodies, Action and Behavior with English Labels (**CVPR 2021**) [[project](https://babel.is.tue.mpg.de/)] [[paper](https://openaccess.thecvf.com/content/CVPR2021/html/Punnakkal_BABEL_Bodies_Action_and_Behavior_With_English_Labels_CVPR_2021_paper.html)] [[code](https://github.com/abhinanda-punnakkal/BABEL)]

- The KIT Motion-Language Dataset (**Big Data 2016**) [[project](https://motion-annotation.humanoids.kit.edu/dataset/)] [[paper](https://matthiasplappert.com/publications/2016_Plappert_Big-Data.pdf)] 

### Audio to Motion

-  AI Choreographer: Music Conditioned 3D Dance Generation with AIST++ (**ICCV 2021**) [[project](https://google.github.io/aichoreographer/)] [[paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Li_AI_Choreographer_Music_Conditioned_3D_Dance_Generation_With_AIST_ICCV_2021_paper.pdf)] [[code](https://github.com/google/aistplusplus_api)]

## Survey

- Human Motion Generation: A Survey [[paper](https://arxiv.org/abs/2307.10894)]
## Papers

### 2024

**ECCV**
- Motion Mamba: Efficient and Long Sequence Motion Generation [[project](https://steve-zeyu-zhang.github.io/MotionMamba/)] [[paper](https://arxiv.org/abs/2403.07487)] [[code](https://github.com/steve-zeyu-zhang/MotionMamba/)]
- BAMM: Bidirectional Autoregressive Motion Model [[project](https://exitudio.github.io/BAMM-page/)] [[paper](https://arxiv.org/abs/2403.19435)] [[code](https://github.com/exitudio/BAMM/)]
- ParCo: Part-Coordinating Text-to-Motion Synthesis [[paper](http://arxiv.org/abs/2403.18512)] [[code](https://github.com/qrzou/ParCo)]

**TPAMI**

- MotionDiffuse: Text-Driven Human Motion Generation with Diffusion Model [[project](https://mingyuan-zhang.github.io/projects/MotionDiffuse.html)] [[paper](https://arxiv.org/abs/2208.15001)] [[code](https://github.com/mingyuan-zhang/MotionDiffuse)]

**CVPR**

- MoMask: Generative Masked Modeling of 3D Human Motions [[project](https://ericguo5513.github.io/momask/)] [[paper](https://arxiv.org/abs/2312.00063)] [[code](https://github.com/EricGuo5513/momask-codes)]
- MMM: Generative Masked Motion Model [[project](https://exitudio.github.io/MMM-page/)] [[paper](http://arxiv.org/abs/2312.03596)] [[code](https://github.com/exitudio/MMM/)]
- FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models [[project](https://shivangi-aneja.github.io/projects/facetalk/)] [[paper](https://arxiv.org/abs/2312.08459)] [[code](https://github.com/shivangi-aneja/FaceTalk)]
- AAMDM: Accelerated Auto-regressive Motion Diffusion Mode [[paper](https://arxiv.org/abs/2401.06146)]
- FlowMDM: Seamless Human Motion Composition with Blended Positional Encodings [[project](https://barquerogerman.github.io/FlowMDM/)] [[paper](https://arxiv.org/abs/2402.15509)] [[code](https://github.com/BarqueroGerman/FlowMDM)]
- OMG: Towards Open-vocabulary Motion Generation via Mixture of Controllers [[project](https://tr3e.github.io/omg-page/)] [[paper](https://arxiv.org/abs/2312.08985)] [[code](https://tr3e.github.io/omg-page/)]
- MAS: Multi-view Ancestral Sampling for 3D motion generation using 2D diffusion [[project](https://guytevet.github.io/mas-page/)] [[paper](https://arxiv.org/abs/2310.14729)] [[code](https://github.com/roykapon/MAS)]
- AnySkill: Learning Open-Vocabulary Physical Skill for Interactive Agents [[project](https://anyskill.github.io/)] [[paper](https://arxiv.org/abs/2403.12835)] [[Video](https://www.youtube.com/watch?v=QojOdY2_dTQ)] [[code](https://github.com/jiemingcui/anyskill)]
- Scaling Up Dynamic Human-Scene Interaction Modeling [[project](https://jnnan.github.io/trumans/)] [[paper](https://arxiv.org/abs/2403.08629)] [[Demo](https://huggingface.co/spaces/jnnan/trumans)] [[code](https://huggingface.co/spaces/jnnan/trumans/tree/main)]
- Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance [[project](https://afford-motion.github.io/)] [[paper](https://arxiv.org/abs/2403.18036)] [[Video](https://www.youtube.com/watch?v=emT0FHDYY1U)] [[code](https://github.com/afford-motion/afford-motion)]

**IJCV**

- InterGen: Diffusion-based Multi-human Motion Generation under Complex Interactions [[project](https://tr3e.github.io/intergen-page/)] [[paper](https://doi.org/10.1007/s11263-024-02042-6)] [[code](https://github.com/tr3e/InterGen)]

**ICLR**

- SinMDM: Single Motion Diffusion [[project](https://sinmdm.github.io/SinMDM-page/)] [[paper](https://arxiv.org/abs/2302.05905)] [[code](https://github.com/SinMDM/SinMDM)]
- Human Motion Diffusion as a Generative Prior [[project](https://priormdm.github.io/priorMDM-page/)] [[paper](https://arxiv.org/abs/2303.01418)] [[code](https://github.com/priorMDM/priorMDM)]
- OmniControl: Control Any Joint at Any Time for Human Motion Generation[[project](https://neu-vi.github.io/omnicontrol/)] [[paper](https://arxiv.org/abs/2310.08580)] [[code](https://github.com/neu-vi/OmniControl)]

**ICML**
- HumanTOMATO: Text-aligned Whole-body Motion Generation [[project](https://lhchen.top/HumanTOMATO/)] [[paper](https://arxiv.org/abs/2310.12978)] [[code](https://github.com/IDEA-Research/HumanTOMATO)]
  

**Siggraph**

- LGTM: Local-to-Global Text-Driven Human Motion Diffusion Model [[paper](http://arxiv.org/abs/2405.03485)] [[code](https://github.com/l-sun/lgtm)]
- Flexible Motion In-betweening with Diffusion Models [[project](https://setarehc.github.io/CondMDI/)] [[paper](http://arxiv.org/abs/2405.11126)] [[code](https://github.com/setarehc/diffusion-motion-inbetweening)]

**arXiv papers**

- BAD: Bidirectional Auto-regressive Diffusion for Text-to-Motion Generation [[project](https://rohollahhs.github.io/BAD-page/)] [[paper](https://arxiv.org/abs/2409.10847)] [[code](https://github.com/rohollahhs/bad/)]
- GUESS: GradUally Enriching SyntheSis for Text-Driven Human Motion Generation [[paper](https://arxiv.org/abs/2401.02142)] [[code](https://github.com/xuehao-gao/guess)]
- Off-the-shelf ChatGPT is a Good Few-shot Human Motion Predictor [[paper](https://arxiv.org/abs/2405.15267)]
- MotionLCM: Real-time Controllable Motion Generation via Latent Consistency Model [[project](https://dai-wenxun.github.io/MotionLCM-page/)] [[paper](https://arxiv.org/abs/2404.19759)] [[code](https://github.com/Dai-Wenxun/MotionLCM)]
- ParCo: Part-Coordinating Text-to-Motion Synthesis [[paper](https://arxiv.org/abs/2403.18512)] [[code](https://github.com/qrzou/ParCo)]
- MotionFix: Text-Driven 3D Human Motion Editing [[paper](https://arxiv.org/abs/2408.00712v1)]
### 2023

**NeurIPS**

- MotionGPT: Human Motion as a Foreign Language [[project](https://motion-gpt.github.io/)] [[paper](https://arxiv.org/abs/2306.14795)] [[code](https://github.com/OpenMotionLab/MotionGPT)]

**ICLR**

- Human Motion Diffusion Model [[project](https://guytevet.github.io/mdm-page/)] [[paper](https://arxiv.org/abs/2209.14916)] [[code](https://github.com/GuyTevet/motion-diffusion-model)]

**ICCV**

- PhysDiff: Physics-Guided Human Motion Diffusion Model [[project](https://nvlabs.github.io/PhysDiff/)] [[paper](https://arxiv.org/abs/2212.02500)]
- GMD: Guided Motion Diffusion for Controllable Human Motion Synthesis [[project](https://korrawe.github.io/gmd-project/)] [[paper](https://arxiv.org/abs/2305.12577)] [[code](https://github.com/korrawe/guided-motion-diffusion)]
- ReMoDiffuse: Retrieval-Augmented Motion Diffusion Model [[project](https://mingyuan-zhang.github.io/projects/ReMoDiffuse.html)] [[paper](https://arxiv.org/abs/2304.01116)] [[code](https://github.com/mingyuan-zhang/ReMoDiffuse)]
- HumanMAC: Masked Motion Completion for Human Motion Prediction [[project](https://lhchen.top/Human-MAC/)] [[paper](https://arxiv.org/pdf/2302.03665.pdf)] [[code](https://github.com/LinghaoChan/HumanMAC)]
- Fg-T2M: Fine-Grained Text-Driven Human Motion Generation via Diffusion Model [[paper](https://arxiv.org/abs/2309.06284)] 
- BeLFusion: Latent Diffusion for Behavior-Driven Human Motion Prediction [[project](https://barquerogerman.github.io/BeLFusion/)] [[paper](https://arxiv.org/abs/2211.14304)] [[code](https://github.com/BarqueroGerman/BeLFusion)]
- Social Diffusion: Long-term Multiple Human Motion Anticipation [[paper](https://openaccess.thecvf.com/content/ICCV2023/papers/Tanke_Social_Diffusion_Long-term_Multiple_Human_Motion_Anticipation_ICCV_2023_paper.pdf)] [[code](https://github.com/jutanke/social_diffusion)]

**AAAI**
- Human Joint Kinematics Diffusion-Refinement for Stochastic Motion Prediction [[paper](https://ojs.aaai.org/index.php/AAAI/article/view/25754)] 

**CVPR**

- MLD: Executing your Commands via Motion Diffusion in Latent Space [[project](https://chenxin.tech/mld/)] [[paper](https://openaccess.thecvf.com/content/CVPR2023/papers/Chen_Executing_Your_Commands_via_Motion_Diffusion_in_Latent_Space_CVPR_2023_paper.pdf)] [[code](https://github.com/chenfengye/motion-latent-diffusion)]
- UDE: A Unified Driving Engine for Human Motion Generation [[project](https://zixiangzhou916.github.io/UDE/)] [[paper](https://arxiv.org/abs/2211.16016)] [[code](https://github.com/zixiangzhou916/UDE)]
- EDGE: Editable Dance Generation From Music [[project](https://edge-dance.github.io/)] [[paper](https://arxiv.org/abs/2211.10658)] [[code](https://github.com/Stanford-TML/EDGE)]
- MoFusion: A Framework for Denoising-Diffusion-based Motion Synthesis [[project](https://vcai.mpi-inf.mpg.de/projects/MoFusion/)] [[paper](https://arxiv.org/abs/2212.04495)]
- T2M-GPT: Generating Human Motion from Textual Descriptions with Discrete Representations [[project](https://mael-zys.github.io/T2M-GPT/)] [[paper](https://arxiv.org/abs/2301.06052)] [[code](https://github.com/Mael-zys/T2M-GPT)]

**MMM**

- DiffMotion: Speech-Driven Gesture Synthesis Using Denoising Diffusion Model [[project](https://zf223669.github.io/DiffMotionWebsite/)] [[paper](https://arxiv.org/abs/2301.10047)] [[code](https://github.com/zf223669/DiffmotionGG-beta)]

**ICASSP**

- Diffusion Motion: Generate Text-Guided 3D Human Motion by Diffusion Model [[paper](https://ieeexplore.ieee.org/document/10096441)] 

**TOG**

- Listen, denoise, action! Audio-driven motion synthesis with diffusion models [[project](https://www.speech.kth.se/research/listen-denoise-action/)] [[paper](https://arxiv.org/abs/2211.09707)] [[code](https://github.com/simonalexanderson/ListenDenoiseAction)]

**arXiv papers**
- DiverseMotion: Towards Diverse Human Motion Generation via Discrete Diffusion [[paper](https://arxiv.org/abs/2309.01372)] [[code](https://github.com/axdfhj/MDD)]
- Text2Performer: Text-Driven Human Video Generation [[project](https://yumingj.github.io/projects/Text2Performer.html)] [[paper](https://arxiv.org/abs/2304.08483)] [[code](https://github.com/yumingj/Text2Performer)]

### 2022

**ECCV** 
- MotionCLIP: Exposing Human Motion Generation to CLIP Space [[project](https://guytevet.github.io/motionclip-page/)] [[paper](http://arxiv.org/abs/2203.08063)] [[code](https://github.com/GuyTevet/MotionCLIP)]
- TEMOS: Generating diverse human motions from textual descriptions [[project](https://mathis.petrovich.fr/temos/)] [[paper](http://arxiv.org/abs/2204.14109)] [[code](https://github.com/Mathux/TEMOS)]

**arXiv papers**

- FLAME: Free-form Language-based Motion Synthesis & Editing [[project](https://kakaobrain.github.io/flame/)] [[paper](https://arxiv.org/abs/2209.00349)] [[code](https://github.com/kakaobrain/flame)]

## Other Resources

- [fengshiwest/Awesome-Motion-Diffusion-Models](https://github.com/fengshiwest/Awesome-Motion-Diffusion-Models)

## Feel free to contact me if you find any interesting paper is missing.
